{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disini preprocessing\n",
    "#Installasi untuk kebutuhan vnv\n",
    "#gensim\n",
    "#ntlk\n",
    "#pyclustering\n",
    "\n",
    "#jangan ubah jadi csv atau excel karena akan berupa koma-koma\n",
    "#Kamis kita coba buat approach Bu Marketing\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "np.warnings = warnings\n",
    "\n",
    "import joblib\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "path= os.path.join(os.getcwd(), 'Scraping_Dataset')\n",
    "data = os.path.join(path,'com.kopikenangan.csv')\n",
    "data = pd.read_csv(data).dropna(subset='content', ignore_index=True)\n",
    "\n",
    "# Seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Download and define stopwords\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Pre-process text and generate tokens\"\"\"\n",
    "    text = str(text).lower()  # Lowercase words\n",
    "    text = re.sub(r\"\\[(.*?)\\]\", \"\", text)  # Remove [+XYZ chars] in content\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove multiple spaces in content\n",
    "    text = re.sub(r\"\\w+…|…\", \"\", text)  # Remove ellipsis (and last word)\n",
    "    #text = re.sub() #melakukan trimp\n",
    "    #text = re.sub() #menghilangkan kata yang hanya memeliki satu saja misal good dan lain-lain\n",
    "    text = re.sub(r\"(?<=\\w)-(?=\\w)\", \" \", text)  # Replace dash between words\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)  # Remove punctuation\n",
    "    text = word_tokenize(text)\n",
    "    text = [word for word in text if word not in stop_words]  # Remove stopwords\n",
    "    return text\n",
    "data['clean_comment']=data['content'].apply(clean_text)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content'].to_excel('googletranslate.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_LDA=pd.read_excel('gt_kopikenangan.xlsx')\n",
    "def clean_text(text):\n",
    "    \"\"\"Pre-process text and generate tokens\"\"\"\n",
    "    text = str(text).lower()  # Lowercase words\n",
    "    text = re.sub(r\"\\[(.*?)\\]\", \"\", text)  # Remove [+XYZ chars] in content\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove multiple spaces in content\n",
    "    text = re.sub(r\"\\w+…|…\", \"\", text)  # Remove ellipsis (and last word)\n",
    "    #text = re.sub() #melakukan trimp\n",
    "    #text = re.sub() #menghilangkan kata yang hanya memeliki satu saja misal good dan lain-lain\n",
    "    text = re.sub(r\"(?<=\\w)-(?=\\w)\", \" \", text)  # Replace dash between words\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)  # Remove punctuation\n",
    "    text = word_tokenize(text)\n",
    "    text = [word for word in text if word not in stop_words]  # Remove stopwords\n",
    "    text = [word for word in text if len(word)>2]\n",
    "    return text\n",
    "data_for_LDA['translate']=data_for_LDA['content'].apply(clean_text)\n",
    "data_for_LDA = data_for_LDA[data_for_LDA['translate'].apply(len) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>content</th>\n",
       "      <th>translate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The application makes the service easier. You...</td>\n",
       "      <td>[application, makes, service, easier, order, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The most favorite hangout place, Kopken at Bl...</td>\n",
       "      <td>[favorite, hangout, place, kopken, blok, plaza]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Very delicious, service ok, discount ok, prom...</td>\n",
       "      <td>[delicious, service, discount, promo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The Taste and Price All Good</td>\n",
       "      <td>[taste, price, good]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>nice coffee</td>\n",
       "      <td>[nice, coffee]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            content  \\\n",
       "1           1   The application makes the service easier. You...   \n",
       "2           2   The most favorite hangout place, Kopken at Bl...   \n",
       "3           3   Very delicious, service ok, discount ok, prom...   \n",
       "4           4                       The Taste and Price All Good   \n",
       "5           5                                        nice coffee   \n",
       "\n",
       "                                           translate  \n",
       "1  [application, makes, service, easier, order, f...  \n",
       "2    [favorite, hangout, place, kopken, blok, plaza]  \n",
       "3              [delicious, service, discount, promo]  \n",
       "4                               [taste, price, good]  \n",
       "5                                     [nice, coffee]  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_for_LDA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kita menggunkaan text Blob disebabkan\n",
    "# polarity mereka \n",
    "# update mereka yang 3 bulan lalu\n",
    "# adapun vader update 2 tahun lalu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                   [good]\n",
       "1        [aplikasinya, mempermudah, pelayanan, bisa, pe...\n",
       "2        [tempat, nongkrong, paling, favoritkopken, yg,...\n",
       "3           [enak, bangetpelayanan, okdiskon, okpromo, ok]\n",
       "4                                [taste, dan, price, good]\n",
       "                               ...                        \n",
       "33570                                              [okeee]\n",
       "33571                                           [oke, lah]\n",
       "33572                                               [good]\n",
       "33573    [kode, referal, di, input, dimana, ini, hmm, t...\n",
       "33574                                     [senang, sekali]\n",
       "Name: clean_comment, Length: 33575, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#disini LDA sentiment analysis \n",
    "\n",
    "#disini lainya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Advan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Advan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Advan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_vector(text):\n",
    "    text = clean_text(text)\n",
    "    word_vectors = [model[word] for word in text if word in model]\n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(300)  # Asumsi bahwa dimensi vektor adalah 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply get_vector to each comment\n",
    "data['vector'] = data['comment'].apply(get_vector)\n",
    "\n",
    "#Melakukan X means Clustering\n",
    "from pyclustering.cluster.xmeans import xmeans, kmeans_plusplus_initializer\n",
    "from pyclustering.cluster.center_initializer import random_center_initializer\n",
    "from pyclustering.utils import timedcall\n",
    "from pyclustering.samples.definitions import SIMPLE_SAMPLES\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Mengambil vektor untuk clustering\n",
    "vectors = data['vector'].tolist()\n",
    "\n",
    "# Inisialisasi center pertama kali (dengan K=2, untuk X-Means akan berkembang sesuai dengan data)\n",
    "initial_centers = kmeans_plusplus_initializer(vectors, 2).initialize()\n",
    "\n",
    "# Inisialisasi X-Means\n",
    "xmeans_instance = xmeans(data=vectors, initial_centers=initial_centers, kmax=20)\n",
    "\n",
    "# Proses clustering\n",
    "xmeans_instance.process()\n",
    "\n",
    "# Mendapatkan hasil clustering\n",
    "clusters = xmeans_instance.get_clusters()\n",
    "centers = xmeans_instance.get_centers()\n",
    "\n",
    "# Menambahkan label cluster ke DataFrame\n",
    "labels = np.zeros(len(data), dtype=int)\n",
    "for cluster_index, cluster in enumerate(clusters):\n",
    "    for data_index in cluster:\n",
    "        labels[data_index] = cluster_index\n",
    "\n",
    "data['cluster'] = labels\n",
    "\n",
    "\n",
    "#mealkukan extraksi untuk kata-kata noun\n",
    "\n",
    "def extract_nouns(text):\n",
    "    tagged_words = nltk.pos_tag(text)\n",
    "    nouns = [word for word, pos in tagged_words if pos in ('NN', 'NNS', 'NNP', 'NNPS')]  # POS tags untuk noun\n",
    "    return nouns\n",
    "\n",
    "data['nouns'] = data['clean_comment'].apply(extract_nouns)\n",
    "#send to joblib\n",
    "joblib.dump(data, 'noun_extrac.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "LDAvis_data_filepath = os.path.join('ldavis_prepared_'+str(total_topics))\n",
    "corpus = [dictionary.doc2bow(text) for text in doc_clean]\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nltk_vnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
