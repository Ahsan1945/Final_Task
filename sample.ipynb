{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disini preprocessing\n",
    "#Installasi untuk kebutuhan vnv\n",
    "#gensim\n",
    "#ntlk\n",
    "#pyclustering\n",
    "\n",
    "#jangan ubah jadi csv atau excel karena akan berupa koma-koma\n",
    "#Kamis kita coba buat approach Bu Marketing\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "np.warnings = warnings\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Download and define stopwords\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "data_for_LDA=pd.read_excel('gt_kopikenangan.xlsx')\n",
    "def clean_text(text):\n",
    "    \"\"\"Pre-process text and generate tokens\"\"\"\n",
    "    text = str(text).lower()  # Lowercase words\n",
    "    text = re.sub(r\"\\[(.*?)\\]\", \"\", text)  # Remove [+XYZ chars] in content\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove multiple spaces in content\n",
    "    text = re.sub(r\"\\w+…|…\", \"\", text)  # Remove ellipsis (and last word)\n",
    "    #text = re.sub() #melakukan trimp\n",
    "    #text = re.sub() #menghilangkan kata yang hanya memeliki satu saja misal good dan lain-lain\n",
    "    text = re.sub(r\"(?<=\\w)-(?=\\w)\", \" \", text)  # Replace dash between words\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)  # Remove punctuation\n",
    "    text = word_tokenize(text)\n",
    "    text = [word for word in text if word not in stop_words]  # Remove stopwords\n",
    "    text = [word for word in text if len(word)>2]\n",
    "    return text\n",
    "data_for_LDA['translate']=data_for_LDA['content'].apply(clean_text)\n",
    "data_for_LDA = data_for_LDA[data_for_LDA['translate'].apply(len) > 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8340\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "ps=SnowballStemmer(\"english\")\n",
    "# stemmed\n",
    "def stemmed_wrapper(term):\n",
    "    return ps.stem(term)\n",
    "\n",
    "term_dict = {}\n",
    "\n",
    "for document in data_for_LDA['translate']:\n",
    "    for term in document:\n",
    "        if term not in term_dict:\n",
    "            term_dict[term] = ' '\n",
    "            \n",
    "print(len(term_dict))\n",
    "\n",
    "for term in term_dict:\n",
    "    term_dict[term] = stemmed_wrapper(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 22318/22318 [00:00<00:00, 301314.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1        [applic, make, servic, easier, order, first, p...\n",
      "2           [favorit, hangout, place, kopken, blok, plaza]\n",
      "3                        [delici, servic, discount, promo]\n",
      "4                                      [tast, price, good]\n",
      "5                                            [nice, coffe]\n",
      "                               ...                        \n",
      "33560                  [profit, everi, purchas, get, pawn]\n",
      "33562    [awesom, old, point, wont, disappear, enter, p...\n",
      "33565                          [point, angus, input, code]\n",
      "33566                               [applic, get, complic]\n",
      "33573             [referr, code, input, hmm, column, fill]\n",
      "Name: textdata_tokens_stemmed, Length: 22318, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import swifter\n",
    "for term in term_dict:\n",
    "    term_dict[term] = stemmed_wrapper(term)\n",
    "    #print(term,\":\" ,term_dict[term])\n",
    "    # untuk melihat hasilnya silahkan jalankan baris di bawah ini\n",
    "\n",
    "# apply stemmed term to dataframe\n",
    "def get_stemmed_term(document):\n",
    "    return [term_dict[term] for term in document]\n",
    "\n",
    "data_for_LDA['textdata_tokens_stemmed'] = data_for_LDA['translate'].swifter.apply(get_stemmed_term)\n",
    "\n",
    "print(data_for_LDA['textdata_tokens_stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 22318 entries, 1 to 33573\n",
      "Data columns (total 4 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   Unnamed: 0               22318 non-null  int64 \n",
      " 1   content                  22318 non-null  object\n",
      " 2   translate                22318 non-null  object\n",
      " 3   textdata_tokens_stemmed  22318 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data_for_LDA.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data_for_LDA)):\n",
    "        a=data_for_LDA.iloc[i][3]\n",
    "        document.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_clean=data_for_LDA['textdata_tokens_stemmed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA model gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word  topic_id  importance  word_count\n",
      "0     applic         0    0.036662        3964\n",
      "1      order         0    0.030605        2713\n",
      "2      promo         0    0.029633        2055\n",
      "3        lot         0    0.029006        1971\n",
      "4     promot         0    0.022272        1517\n",
      "5        get         0    0.020097        1031\n",
      "6        use         0    0.020013        2534\n",
      "7       cool         0    0.017777         888\n",
      "8      point         0    0.012754         624\n",
      "9       time         0    0.011894         772\n",
      "10      good         1    0.126738        4186\n",
      "11      easi         1    0.080761        2341\n",
      "12    applic         1    0.061674        3964\n",
      "13    servic         1    0.053204        1898\n",
      "14       use         1    0.045713        2534\n",
      "15       app         1    0.036526        2234\n",
      "16    friend         1    0.032020        1132\n",
      "17     price         1    0.029426         931\n",
      "18      fast         1    0.025744         871\n",
      "19      okay         1    0.020325         658\n",
      "20     coffe         2    0.142342        7446\n",
      "21    realli         2    0.050890        2584\n",
      "22     great         2    0.042806        2347\n",
      "23    delici         2    0.034996        2025\n",
      "24      like         2    0.034546        1829\n",
      "25      kopi         2    0.030152        1383\n",
      "26  kenangan         2    0.029215        1345\n",
      "27    memori         2    0.028501        1549\n",
      "28      make         2    0.027528        1356\n",
      "29     order         2    0.027097        2713\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "total_topics = 3 # jumlah topik yang akan di extract\n",
    "number_words = 10 # jumlah kata per topik\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "lda_model = Lda(doc_term_matrix, num_topics=total_topics, id2word = dictionary, passes=50)\n",
    "\n",
    "lda_model.show_topics(num_topics=total_topics, num_words=number_words)\n",
    "# Word Count of Topic Keywords\n",
    "\n",
    "from collections import Counter\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "data_flat = [w for w_list in doc_clean for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i , weight, counter[word]])\n",
    "\n",
    "df_imp_wcount = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count']) \n",
    "print(df_imp_wcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import swifter\n",
    "#Dominant topic and its percentage contribution in each topic\n",
    "def format_topics_sentences(ldamodel=None, corpus=doc_term_matrix, texts=document):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame(columns=['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords'])\n",
    "   \n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df.loc[len(sent_topics_df)] = [int(topic_num), round(prop_topic, 4), topic_keywords]\n",
    "            else:\n",
    "                break\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1, ignore_index=True)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
      "0            0             0.0              0.6029   \n",
      "1            1             1.0              0.5402   \n",
      "2            2             0.0              0.4667   \n",
      "3            3             1.0              0.8263   \n",
      "4            4             2.0              0.4445   \n",
      "5            5             1.0              0.4444   \n",
      "6            6             0.0              0.8061   \n",
      "7            7             0.0              0.5897   \n",
      "8            8             1.0              0.7776   \n",
      "9            9             0.0              0.5075   \n",
      "\n",
      "                                            Keywords  \\\n",
      "0  applic, order, promo, lot, promot, get, use, c...   \n",
      "1  good, easi, applic, servic, use, app, friend, ...   \n",
      "2  applic, order, promo, lot, promot, get, use, c...   \n",
      "3  good, easi, applic, servic, use, app, friend, ...   \n",
      "4  coffe, realli, great, delici, like, kopi, kena...   \n",
      "5  good, easi, applic, servic, use, app, friend, ...   \n",
      "6  applic, order, promo, lot, promot, get, use, c...   \n",
      "7  applic, order, promo, lot, promot, get, use, c...   \n",
      "8  good, easi, applic, servic, use, app, friend, ...   \n",
      "9  applic, order, promo, lot, promot, get, use, c...   \n",
      "\n",
      "                                                Text  \n",
      "0                                                NaN  \n",
      "1  [applic, make, servic, easier, order, first, p...  \n",
      "2     [favorit, hangout, place, kopken, blok, plaza]  \n",
      "3                  [delici, servic, discount, promo]  \n",
      "4                                [tast, price, good]  \n",
      "5                                      [nice, coffe]  \n",
      "6                                      [good, coffe]  \n",
      "7  [annoy, applic, cant, make, transact, via, ovo...  \n",
      "8                                                NaN  \n",
      "9                         [must, instal, coffe, app]  \n"
     ]
    }
   ],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=doc_term_matrix, texts=doc_clean)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "print(df_dominant_topic.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic.to_csv('df_dominant_topic.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "import os\n",
    "LDAvis_data_filepath = os.path.join('ldavis_prepared_'+str(total_topics))\n",
    "corpus = [dictionary.doc2bow(text) for text in doc_clean]\n",
    "# proses ini mungkin agak lama\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1568828985232384806394267984\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1568828985232384806394267984_data = {\"mdsDat\": {\"x\": [0.011013721235393662, -0.2837687844301856, 0.272755063194792], \"y\": [-0.28069336305826775, 0.13201421257957638, 0.1486791504786913], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [42.608778219688375, 34.736663590717455, 22.65455818959417]}, \"tinfo\": {\"Term\": [\"coffe\", \"good\", \"easi\", \"realli\", \"servic\", \"great\", \"delici\", \"use\", \"like\", \"kopi\", \"friend\", \"promo\", \"kenangan\", \"lot\", \"memori\", \"app\", \"make\", \"price\", \"fast\", \"promot\", \"applic\", \"get\", \"okay\", \"tast\", \"cool\", \"queue\", \"easier\", \"order\", \"help\", \"drink\", \"promo\", \"lot\", \"promot\", \"get\", \"cool\", \"point\", \"discount\", \"time\", \"outlet\", \"pick\", \"pleas\", \"even\", \"cant\", \"often\", \"payment\", \"via\", \"free\", \"long\", \"wait\", \"first\", \"still\", \"voucher\", \"cashback\", \"increas\", \"updat\", \"come\", \"give\", \"pay\", \"code\", \"though\", \"better\", \"order\", \"mani\", \"applic\", \"buy\", \"use\", \"app\", \"dont\", \"hope\", \"want\", \"also\", \"coffe\", \"realli\", \"great\", \"delici\", \"like\", \"memori\", \"kopi\", \"kenangan\", \"make\", \"easier\", \"queue\", \"drink\", \"best\", \"favorit\", \"love\", \"right\", \"lover\", \"recommend\", \"feel\", \"memor\", \"queu\", \"sugar\", \"addict\", \"enjoy\", \"top\", \"wow\", \"variant\", \"high\", \"rememb\", \"ex\", \"suitabl\", \"bother\", \"alway\", \"tast\", \"awesom\", \"order\", \"need\", \"help\", \"without\", \"success\", \"want\", \"applic\", \"dont\", \"thank\", \"buy\", \"good\", \"easi\", \"servic\", \"friend\", \"price\", \"fast\", \"okay\", \"nice\", \"qualiti\", \"afford\", \"simpl\", \"quit\", \"practic\", \"satisfi\", \"product\", \"cheap\", \"job\", \"far\", \"improv\", \"comfort\", \"maintain\", \"understand\", \"overal\", \"everyth\", \"smooth\", \"quick\", \"luck\", \"attract\", \"serv\", \"excel\", \"experi\", \"user\", \"app\", \"use\", \"also\", \"applic\", \"help\", \"place\", \"tast\", \"keep\", \"thank\"], \"Freq\": [6849.0, 3977.0, 2534.0, 2449.0, 1670.0, 2060.0, 1684.0, 2615.0, 1662.0, 1451.0, 1005.0, 1749.0, 1406.0, 1712.0, 1371.0, 1829.0, 1325.0, 924.0, 808.0, 1315.0, 5014.0, 1186.0, 638.0, 1669.0, 1049.0, 822.0, 763.0, 3110.0, 1181.0, 706.0, 1748.9243814922172, 1711.9241724605292, 1314.483442028311, 1186.1146138222546, 1049.1878857219724, 752.747479288042, 690.7062778088169, 701.9850771517374, 604.4647253948895, 527.4238937521724, 516.1981910289327, 538.0076550455444, 437.09458869088326, 452.6789412562019, 398.81841182351184, 407.1568072139124, 396.81068902115095, 386.3604662759305, 362.6673914313255, 366.34875706252774, 362.52242007065456, 313.3991699465768, 295.93706037313666, 292.8849666367746, 275.2004393909345, 302.2516280858486, 277.2949251479188, 266.88287590411477, 266.69670794256314, 252.9721661676375, 265.38984952689987, 1806.2686208165937, 471.00912009943386, 2163.7354017223306, 607.8226743488642, 1181.1205634382397, 682.9380114946474, 468.62768723784956, 329.83117211894256, 345.752490343633, 346.80353957917777, 6848.736108020372, 2448.54983581039, 2059.5913089196415, 1683.8210188832174, 1662.1623627592098, 1371.301586647661, 1450.7367939063274, 1405.651801212291, 1324.5242845913137, 762.3980490626587, 821.5056481583958, 706.1277545319483, 626.6363102280638, 451.02267101153166, 380.4303470224304, 313.3885970543368, 276.98559590806883, 286.56930702629165, 247.70581192315458, 185.8402450660232, 202.46597691059185, 154.7496282250919, 149.46477990723446, 160.28461936818258, 159.14295133726557, 140.89629308897753, 136.29138649612042, 133.94362428945604, 124.52471459995846, 111.07779854023072, 126.27233119179925, 135.83988816277025, 694.9178069782464, 1145.8106622766336, 628.7881501869562, 1303.758430769236, 399.5822648623459, 595.1240275739847, 306.32430556187677, 241.67345900784477, 360.8590038890968, 915.8294675047242, 338.1266907105702, 269.350240746878, 267.42124288031147, 3976.9605807368143, 2534.2362677300107, 1669.527861016142, 1004.7570875221103, 923.3734393962537, 807.8401513187646, 637.7876686827029, 410.33148038222976, 372.0089696957648, 357.5360604489617, 318.56362734468604, 265.13722243354346, 270.10941094568335, 238.40867714469053, 235.68726766274656, 228.51792697578563, 203.86631910993646, 217.65540558348746, 198.39665749391673, 155.67018377895334, 119.73344713722962, 120.53824449785196, 105.10317886984546, 108.41747926645893, 86.34251029137472, 95.95540174954894, 74.35118012399012, 67.30895846382317, 72.80537505864166, 55.94791554070432, 60.805978944617564, 316.0911516796813, 1146.154067294699, 1434.4623287946345, 593.5773384182935, 1935.294373409788, 585.9193065729787, 198.4463098248606, 523.0616182069391, 208.8694416964166, 212.069036148972], \"Total\": [6849.0, 3977.0, 2534.0, 2449.0, 1670.0, 2060.0, 1684.0, 2615.0, 1662.0, 1451.0, 1005.0, 1749.0, 1406.0, 1712.0, 1371.0, 1829.0, 1325.0, 924.0, 808.0, 1315.0, 5014.0, 1186.0, 638.0, 1669.0, 1049.0, 822.0, 763.0, 3110.0, 1181.0, 706.0, 1749.5773231587432, 1712.5700898563707, 1315.133227307313, 1186.7703961807783, 1049.9523249031224, 753.3688685419696, 691.3410029034981, 702.6419507342914, 605.1579767896866, 528.0439253316282, 516.8270431600264, 538.6662758737945, 437.7115333585763, 453.31992018355845, 399.44457444613806, 407.8001074135749, 397.47825086238885, 387.0369781725918, 363.30466097831504, 366.99426342849614, 363.18789420889124, 314.0278382264438, 296.5664776876585, 293.51919169442715, 275.8022926575455, 302.91493203969253, 277.9231393891933, 267.49415962345336, 267.3308214439433, 253.5893605801974, 266.0387479498111, 3110.440398245288, 560.7755516980186, 5014.859242636843, 875.6046427365737, 2615.8735686418686, 1829.3971876680926, 807.1137850167012, 466.16513654560737, 706.9785880148222, 1050.2900998648302, 6849.458480422269, 2449.2962734377934, 2060.327665786825, 1684.5233921058652, 1662.9069837074958, 1371.9904674947331, 1451.519780901718, 1406.4479039537155, 1325.3504391562649, 763.1357496305632, 822.3269920657542, 706.8909264328281, 627.331792089468, 451.7073699777828, 381.12446231057726, 314.1448573875724, 277.6852207579147, 287.32504450351314, 248.40884647464955, 186.54307313446702, 203.3775395120921, 155.4469189766189, 150.14258984108648, 161.01310119483173, 159.91620922052203, 141.64464474939922, 137.03323966638334, 134.69118361880652, 125.22628972107442, 111.7414338314326, 127.03308563388546, 136.87685144200734, 854.0282301265114, 1669.2109630884524, 829.8123053636432, 3110.440398245288, 623.6053828673685, 1181.4912976643313, 445.9493898682456, 335.633774010529, 706.9785880148222, 5014.859242636843, 807.1137850167012, 678.302435860742, 875.6046427365737, 3977.590267524837, 2534.899263037089, 1670.1440850517004, 1005.3803272080296, 924.0496409490919, 808.454553021606, 638.4368468261516, 410.9388201852927, 372.6276349239854, 358.17302413170665, 319.1817611011537, 265.80454188778657, 270.79657515459854, 239.0287730231788, 236.33246556362724, 229.16453311291718, 204.47047612821217, 218.33638515824686, 199.09508109705854, 156.3037681147901, 120.36598810182275, 121.19891087386696, 105.75546053610675, 109.10036401784535, 86.98250288898555, 96.68479425804536, 74.9687916245842, 67.95386532652132, 73.56261771211592, 56.561676569341074, 61.47508319659566, 358.46982843586227, 1829.3971876680926, 2615.8735686418686, 1050.2900998648302, 5014.859242636843, 1181.4912976643313, 284.1832612696283, 1669.2109630884524, 420.56986085565075, 678.302435860742], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.5189, -3.5402, -3.8044, -3.9072, -4.0298, -4.3619, -4.4479, -4.4317, -4.5813, -4.7176, -4.7391, -4.6977, -4.9055, -4.8704, -4.9971, -4.9764, -5.0022, -5.0288, -5.0921, -5.082, -5.0925, -5.2381, -5.2955, -5.3058, -5.3681, -5.2743, -5.3605, -5.3988, -5.3995, -5.4523, -5.4044, -3.4866, -4.8307, -3.306, -4.5757, -3.9114, -4.4592, -4.8358, -5.187, -5.1399, -5.1369, -1.9495, -2.9781, -3.1511, -3.3525, -3.3655, -3.5578, -3.5015, -3.5331, -3.5925, -4.1449, -4.0702, -4.2215, -4.341, -4.6698, -4.84, -5.0339, -5.1574, -5.1234, -5.2691, -5.5565, -5.4708, -5.7395, -5.7743, -5.7044, -5.7115, -5.8333, -5.8665, -5.8839, -5.9568, -6.0711, -5.9429, -5.8699, -4.2376, -3.7375, -4.3376, -3.6083, -4.7909, -4.3926, -5.0567, -5.2938, -4.8929, -3.9615, -4.9579, -5.1853, -5.1925, -2.0656, -2.5163, -2.9336, -3.4414, -3.5259, -3.6595, -3.8959, -4.3369, -4.435, -4.4747, -4.5901, -4.7737, -4.7551, -4.8799, -4.8914, -4.9223, -5.0364, -4.971, -5.0636, -5.3062, -5.5686, -5.5619, -5.699, -5.6679, -5.8956, -5.79, -6.0451, -6.1446, -6.0661, -6.3295, -6.2462, -4.5979, -3.3097, -3.0854, -3.9677, -2.7859, -3.9807, -5.0634, -4.0942, -5.0122, -4.997], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.8527, 0.8527, 0.8526, 0.8526, 0.8524, 0.8523, 0.8522, 0.8522, 0.852, 0.8519, 0.8519, 0.8519, 0.8517, 0.8517, 0.8515, 0.8515, 0.8514, 0.8514, 0.8514, 0.8513, 0.8513, 0.8511, 0.851, 0.8509, 0.8509, 0.8509, 0.8508, 0.8508, 0.8507, 0.8507, 0.8507, 0.3096, 0.6787, 0.0125, 0.4881, 0.058, -0.1322, 0.3095, 0.5072, 0.1378, -0.255, 1.0573, 1.0571, 1.057, 1.057, 1.0569, 1.0569, 1.0568, 1.0568, 1.0568, 1.0564, 1.0564, 1.0563, 1.0563, 1.0559, 1.0556, 1.055, 1.0549, 1.0547, 1.0545, 1.0536, 1.0529, 1.0529, 1.0528, 1.0528, 1.0525, 1.0521, 1.0519, 1.0518, 1.0518, 1.0514, 1.0514, 1.0498, 0.8512, 0.6811, 0.78, 0.1879, 0.6123, 0.3716, 0.6818, 0.7289, 0.3849, -0.643, 0.1873, 0.1338, -0.1287, 1.4847, 1.4845, 1.4844, 1.4842, 1.4841, 1.484, 1.4838, 1.4833, 1.4831, 1.483, 1.4829, 1.4823, 1.4823, 1.4822, 1.4821, 1.482, 1.4818, 1.4817, 1.4813, 1.4807, 1.4795, 1.4793, 1.4786, 1.4785, 1.4774, 1.4772, 1.4765, 1.4753, 1.4745, 1.4739, 1.4739, 1.359, 1.0172, 0.884, 0.9142, 0.5327, 0.7835, 1.1257, 0.3244, 0.7849, 0.3221]}, \"token.table\": {\"Topic\": [2, 3, 1, 2, 3, 1, 2, 1, 3, 1, 2, 3, 3, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 3, 1, 2, 1, 3, 1, 2, 1, 1, 2, 2, 3, 2, 2, 1, 3, 2, 3, 3, 3, 3, 2, 2, 1, 1, 3, 1, 1, 3, 2, 2, 3, 2, 1, 3, 3, 1, 3, 1, 3, 2, 2, 2, 1, 1, 2, 2, 3, 3, 2, 1, 2, 3, 2, 2, 1, 2, 3, 1, 3, 1, 2, 1, 3, 1, 1, 1, 2, 3, 1, 1, 3, 3, 3, 1, 1, 3, 1, 2, 2, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 1, 1, 2, 3, 2, 2, 2, 3, 1, 2, 3, 1, 1, 2, 3, 1, 1, 3, 1, 3, 2, 1, 1, 1, 1, 2, 1, 2, 2], \"Freq\": [0.9923899684806569, 0.9995169258429607, 0.33038490988790437, 0.10473296855236162, 0.5655580301827527, 0.18617651547238262, 0.813790429266075, 0.3733470263341832, 0.626435859705672, 0.4315175950705559, 0.18265717055666786, 0.38585330243138893, 0.985963045340571, 0.24222345065359943, 0.7580027386125077, 0.9994711058906119, 0.996095501283869, 0.007305837250527968, 0.9935938660718037, 0.6943773140578439, 0.3049321428510597, 0.9983744240113651, 0.9980898795707615, 0.9992820306411198, 0.9987625016743059, 0.9999330632598797, 0.9969795743196553, 0.9980565528364804, 0.9990929827188008, 0.9996892936552155, 0.9995067515132678, 0.5810828766730768, 0.41877614566204685, 0.9987396550167591, 0.9996452470320215, 0.99851173316004, 0.9937079580027103, 0.9987631008220931, 0.9899142039740091, 0.9933647367318457, 0.9900696619441169, 0.9922719389402637, 0.9984593261539846, 0.9994377506813376, 0.9984340083319483, 0.9983541388301915, 0.9972907929971231, 0.9987967873428264, 0.9996217081260325, 0.999350846479439, 0.9966784363791293, 0.9998516017273935, 0.999840964234832, 0.503600831572983, 0.49598334000297145, 0.9948683826198851, 0.7079036464317713, 0.2917421088324875, 0.994499707923348, 0.9982311490726383, 0.9976990510457991, 0.5017002396955402, 0.4969447871865778, 0.9996815353398755, 0.9996419057400685, 0.9994545794104047, 0.997320725845143, 0.999667114438266, 0.997049619161782, 0.9975323830485309, 0.9870774010946375, 0.996959372763067, 0.9997355875503479, 0.8399082281205381, 0.14265957165529308, 0.017832446456911635, 0.997088752075637, 0.9992780799005537, 0.3592015177451434, 0.6414312816877561, 0.9977154258999689, 0.9992942728318029, 0.9993157556172967, 0.580625174820527, 0.4192332380763938, 0.9980864884309556, 0.9928565340051748, 0.998152633970966, 0.9988870184386545, 0.9980230331577575, 0.2991027677712285, 0.6967335061023912, 0.9983997680249671, 0.9995103745889002, 0.9970584001878762, 0.9988640859726824, 0.998593229403187, 0.9996700213525282, 0.9991383174846602, 0.9983156511617464, 0.004916963802389514, 0.9932266880826818, 0.9996023576157548, 0.9929172496740523, 0.9969731823163269, 0.9998790373214517, 0.9988687219936752, 0.9981929535596842, 0.9963556386149592, 0.9956960285150315, 0.9923518530251642, 0.9999137289692608, 0.999430540452792, 0.9887045916551803, 0.9994826528860481, 0.2443139110232322, 0.7210239813124657, 0.03575325527169251, 0.9971249415584357, 0.9918675860801897, 0.6865519250362561, 0.3133216900470872, 0.2904309192845724, 0.3965782603428933, 0.31254494867172256, 0.9976759254455748, 0.9990863757371439, 0.9942706919768303, 0.9983588064246388, 0.9970910587804951, 0.4514744191605413, 0.5481916317326132, 0.1171646723610232, 0.8815246777638889, 0.9924599340357213, 0.9980379911652072, 0.9967269200327945, 0.9991614173693928, 0.48940661834124166, 0.5106236682693301, 0.31169456256250766, 0.6861765190224989, 0.9954488590052964], \"Term\": [\"addict\", \"afford\", \"also\", \"also\", \"also\", \"alway\", \"alway\", \"app\", \"app\", \"applic\", \"applic\", \"applic\", \"attract\", \"awesom\", \"awesom\", \"best\", \"better\", \"bother\", \"bother\", \"buy\", \"buy\", \"cant\", \"cashback\", \"cheap\", \"code\", \"coffe\", \"come\", \"comfort\", \"cool\", \"delici\", \"discount\", \"dont\", \"dont\", \"drink\", \"easi\", \"easier\", \"enjoy\", \"even\", \"everyth\", \"ex\", \"excel\", \"experi\", \"far\", \"fast\", \"favorit\", \"feel\", \"first\", \"free\", \"friend\", \"get\", \"give\", \"good\", \"great\", \"help\", \"help\", \"high\", \"hope\", \"hope\", \"improv\", \"increas\", \"job\", \"keep\", \"keep\", \"kenangan\", \"kopi\", \"like\", \"long\", \"lot\", \"love\", \"lover\", \"luck\", \"maintain\", \"make\", \"mani\", \"mani\", \"mani\", \"memor\", \"memori\", \"need\", \"need\", \"nice\", \"often\", \"okay\", \"order\", \"order\", \"outlet\", \"overal\", \"pay\", \"payment\", \"pick\", \"place\", \"place\", \"pleas\", \"point\", \"practic\", \"price\", \"product\", \"promo\", \"promot\", \"qualiti\", \"queu\", \"queu\", \"queue\", \"quick\", \"quit\", \"realli\", \"recommend\", \"rememb\", \"right\", \"satisfi\", \"serv\", \"servic\", \"simpl\", \"smooth\", \"still\", \"success\", \"success\", \"success\", \"sugar\", \"suitabl\", \"tast\", \"tast\", \"thank\", \"thank\", \"thank\", \"though\", \"time\", \"top\", \"understand\", \"updat\", \"use\", \"use\", \"user\", \"user\", \"variant\", \"via\", \"voucher\", \"wait\", \"want\", \"want\", \"without\", \"without\", \"wow\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 3, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1568828985232384806394267984\", ldavis_el1568828985232384806394267984_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1568828985232384806394267984\", ldavis_el1568828985232384806394267984_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1568828985232384806394267984\", ldavis_el1568828985232384806394267984_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "0      0.011014 -0.280693       1        1  42.608778\n",
       "2     -0.283769  0.132014       2        1  34.736664\n",
       "1      0.272755  0.148679       3        1  22.654558, topic_info=       Term         Freq        Total Category  logprob  loglift\n",
       "24    coffe  6849.000000  6849.000000  Default  30.0000  30.0000\n",
       "21     good  3977.000000  3977.000000  Default  29.0000  29.0000\n",
       "66     easi  2534.000000  2534.000000  Default  28.0000  28.0000\n",
       "124  realli  2449.000000  2449.000000  Default  27.0000  27.0000\n",
       "10   servic  1670.000000  1670.000000  Default  26.0000  26.0000\n",
       "..      ...          ...          ...      ...      ...      ...\n",
       "97     help   585.919307  1181.491298   Topic3  -3.9807   0.7835\n",
       "16    place   198.446310   284.183261   Topic3  -5.0634   1.1257\n",
       "23     tast   523.061618  1669.210963   Topic3  -4.0942   0.3244\n",
       "99     keep   208.869442   420.569861   Topic3  -5.0122   0.7849\n",
       "259   thank   212.069036   678.302436   Topic3  -4.9970   0.3221\n",
       "\n",
       "[157 rows x 6 columns], token_table=      Topic      Freq     Term\n",
       "term                          \n",
       "641       2  0.992390   addict\n",
       "315       3  0.999517   afford\n",
       "103       1  0.330385     also\n",
       "103       2  0.104733     also\n",
       "103       3  0.565558     also\n",
       "...     ...       ...      ...\n",
       "244       1  0.489407     want\n",
       "244       2  0.510624     want\n",
       "245       1  0.311695  without\n",
       "245       2  0.686177  without\n",
       "795       2  0.995449      wow\n",
       "\n",
       "[143 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 3, 2])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# proses ini mungkin agak lama\n",
    "LDAvis_prepared"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nltk_vnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
